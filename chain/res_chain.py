from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser


def create_response_chain():
    """
    1. ì£¼ì–´ì§„ ì •ë³´ì— ë§ê²Œ ìŠ¤í¬ì¸  ë‰´ìŠ¤ or DB ê²€ìƒ‰
    2. í•´ë‹¹ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ì ˆí•œ ë‹µë³€
    """
    prompt = PromptTemplate.from_template(
        # ì˜¤ëŠ˜ ë‚ ì§œë¥¼ í™•ì¸í•˜ê³  í•´ë‹¹ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê°€ìƒìœ¼ë¡œ ì§€ì–´ë‚´ì„œ ë‹µë³€í•´ì¤˜.
        """
        ë‹¹ì‹ ì€ ìŠ¤í¬ì¸  ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ í•µì‹¬ ì •ë³´ë¥¼ ì •ë¦¬í•˜ì—¬ **Markdown í˜•ì‹ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì¶œë ¥**í•˜ì„¸ìš”.

        **ì¶œë ¥ ê·œì¹™:**  
        - ì œëª©ì„ í™œìš©í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”.  
        - ë¦¬ìŠ¤íŠ¸(`-`), êµµì€ ê¸€ì”¨(`**`), ì´ëª¨ì§€(`âš½`, `ğŸ“…`, `ğŸŸï¸`) ë“±ì„ í™œìš©í•˜ì„¸ìš”.  
        - ë¶ˆí•„ìš”í•œ ì„¤ëª…ì€ ìµœì†Œí™”í•˜ê³ , **í•µì‹¬ ì •ë³´ë§Œ ìš”ì•½**í•˜ì„¸ìš”.  
        - ëª¨ë“  ë‹µë³€ì„ ì¼ê´€ëœ Markdown ìŠ¤íƒ€ì¼ë¡œ ì œê³µí•˜ì„¸ìš”.  

        **REFERENCE:**  
        {reference}

        **QUESTION:**  
        {question}
        """
    )
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
    answer_parser = StrOutputParser()
    chain = prompt | llm | answer_parser
    return chain


def predict_response_chain():
    """
    í˜„ì¬ëŠ” ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì§€ì–´ì„œ ë‹µë³€
    í–¥í›„ ì˜ˆì¸¡ ëª¨ë¸ ì—°ë™
    """
    prompt = PromptTemplate.from_template(
        #
        """
        ë‹¹ì‹ ì€ ìŠ¤í¬ì¸  ê²½ê¸°ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì˜¤ëŠ˜ ë‚ ì§œë¥¼ í™•ì¸í•˜ê³  í•´ë‹¹ ì •ë³´ê°€ ì—†ìœ¼ë©´ ê°€ìƒìœ¼ë¡œ ì§€ì–´ë‚´ì„œ ë‹µë³€í•˜ì„¸ìš”.
        ëª¨ë“  ë‹µë³€ì„ ì¼ê´€ëœ Markdown ìŠ¤íƒ€ì¼ë¡œ ì œê³µí•˜ì„¸ìš”.  

        **QUESTION:**  
        {question}
        """
    )
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
    answer_parser = StrOutputParser()
    chain = prompt | llm | answer_parser
    return chain


def generate_natural_language_answer():
    """
    ì¿¼ë¦¬ ê²°ê³¼ì™€ ì»¬ëŸ¼ ì •ë³´ë¥¼ ë°›ì•„ LLMì„ í†µí•´ ìì—°ì–´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ì€ ë¬¸ìì—´ë¡œ ë³€í™˜
    prompt = PromptTemplate.from_template(
        """
        ì•„ë˜ëŠ” SQL ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼ì…ë‹ˆë‹¤.

        ì»¬ëŸ¼: {columns}
        ê²°ê³¼:
        {result}

        ìœ„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì´í•´í•˜ê¸° ì‰¬ìš´ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ì„ ì‘ì„±í•´ì£¼ì„¸ìš”. (2ì¤„ ì´ë‚´ë¡œ)
        ì°¸ê³ ë¡œ win_rateëŠ” AI ì˜ˆì¸¡ ìŠ¹ë¥ ì´ë‹¤.
        ê²°ê³¼ê°€ ì—†ì„ ê²½ìš° ì‚¬ìš©ì ì§ˆë¬¸: {question}ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ê³  ë§í•´ì¤˜.
        ì‘ì„±ëœ ë‹µë³€:
        """
    )
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

    # nl_prompt = PromptTemplate(template=nl_prompt_template, input_variables=["columns", "result"])
    # nl_chain = LLMChain(llm=llm, prompt=nl_prompt)
    answer_parser = StrOutputParser()
    chain = prompt | llm | answer_parser
    return chain
